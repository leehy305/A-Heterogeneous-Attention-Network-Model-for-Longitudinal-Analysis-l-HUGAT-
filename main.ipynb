{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eec08b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import import_ipynb\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import plotly.graph_objects as go\n",
    "import copy\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463ac172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed = 100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "class EarlyStopping:\n",
    "    \"\"\"Stop training early if validation loss does not improve after given patience\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''When validation loss begin to decrease, save model'''\n",
    "        #if self.verbose:\n",
    "        #    print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f3485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' calulate loss'''\n",
    "def get_mob_loss(emb, src_matrix, dst_matrix):  \n",
    "    prediction_src, prediction_dst = predict_distribution(emb, emb)\n",
    "    src_loss = -torch.multiply(torch.Tensor(src_matrix).to('cuda:0'), prediction_src).sum(-1).sum(-1)\n",
    "    dst_loss = -torch.multiply(torch.Tensor(dst_matrix).to('cuda:0'), prediction_dst).sum(-1).sum(-1)\n",
    "    return src_loss + dst_loss   \n",
    "\n",
    "def predict_distribution(emb_src, emb_dst):\n",
    "    src = pairwise_inner_product(emb_src, emb_dst).T\n",
    "    p = nn.LogSoftmax(dim=1)\n",
    "    prediction_src = p(src)\n",
    "    dst = pairwise_inner_product(emb_dst, emb_src).T\n",
    "    q = nn.LogSoftmax(dim=1)\n",
    "    prediction_dst = q(dst)\n",
    "    return prediction_src, prediction_dst\n",
    "\n",
    "def pairwise_inner_product(a, b):\n",
    "    n, _ = list(a.size())\n",
    "    b_ = torch.unsqueeze(b, 0)\n",
    "    b_ = torch.tile(b_, [n, 1, 1])\n",
    "    b_ = b_.permute(1, 0, 2)\n",
    "    inner_product = torch.multiply(b_, a)\n",
    "    inner_product = torch.sum(inner_product, axis=-1)\n",
    "    return inner_product\n",
    "\n",
    "def get_loss(embs, data):\n",
    "    mob_loss = 0\n",
    "    i = 0\n",
    "    for year in range(2018, 2022):\n",
    "        mob_loss += get_mob_loss(embs[i*77:(i+1)*77, :], data['src_matrix{}'.format(year)], data['dst_matrix{}'.format(year)])\n",
    "        i += 1\n",
    "    mob_loss = mob_loss / 4\n",
    "    return mob_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30386d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed_no, k, patience, args):\n",
    "    device = torch.device(args['device'])\n",
    "    \n",
    "    # set random seed for reproducibility\n",
    "    seed(seed_no)\n",
    "    \n",
    "    # load dataset\n",
    "    data = utils.load_dataset(k=k)\n",
    "\n",
    "    # Downstream Applications\n",
    "    income = data['income']\n",
    "    unemployed = data['unemployed']\n",
    "    education = data['education']\n",
    "    poverty = data['poverty']\n",
    "    white = data['white']\n",
    "    black = data['black']\n",
    "    hispanic = data['hispanic']\n",
    "    value = data['value']\n",
    "    \n",
    "    # input node feature\n",
    "    feats = data['feats'].to(args['device'])\n",
    "\n",
    "    # Heterogenous Urban Graph (HUG)\n",
    "    for year in range(2018,2022):\n",
    "        g_ = data['heterograph_unified{}'.format(year)]\n",
    "        g_ = g_[0][0].to(args['device'])\n",
    "        data['heterograph_unified{}'.format(year)] = g_\n",
    "    \n",
    "    # Longitudinal Heterogenous Urban Graph (l-HUG)\n",
    "    g_total = data['heterograph_unified_3'][0][0].to(args['device'])\n",
    "    data['heterograph_unified_3'] = g_total\n",
    "    \n",
    "    # metapath for urban-HIN\n",
    "    meta_paths = data['meta_paths']\n",
    "\n",
    "    from model import HUGAT_J\n",
    "    model = HUGAT_J(data, meta_paths=meta_paths,\n",
    "                    in_size=feats.shape[-1],\n",
    "                    hidden_size = args['hidden_size'],\n",
    "                    out_size=args['out_size'],\n",
    "                    num_heads=args['num_heads'],\n",
    "                    dropout=args['dropout'])\n",
    "    model.to(args['device'])\n",
    "\n",
    "    # params\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"total_params: {:d}\".format(total_params))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'],weight_decay=args['weight_decay'])\n",
    "    \n",
    "    # Early stopping \n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True)\n",
    "\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(data, feats)\n",
    "        loss = get_loss(pred, data)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args['grad_norm'])\n",
    "        optimizer.step()\n",
    "        if (epoch+1)%100 ==0:\n",
    "            print(\"Epoch {:04d} | Loss = {:.4f}\".format(epoch+1, loss))\n",
    "            \n",
    "        early_stopping(loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "            \n",
    "    print(\"Epoch {:04d} | Loss = {:.4f}\".format(epoch+1, loss))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss = get_loss(pred, data)\n",
    "    emb, beta = model.forward(data, feats)\n",
    "    emb = emb.detach().cpu().numpy()\n",
    "    \n",
    "    print('emb size:', emb.shape)\n",
    "    \n",
    "    mae_income, rmse_income, r2_income = utils.predict_regression(emb, income)\n",
    "    mae_unemployed, rmse_unemployed, r2_unemployed = utils.predict_regression(emb, unemployed)\n",
    "    mae_education, rmse_education, r2_education = utils.predict_regression(emb, education)\n",
    "    mae_poverty, rmse_poverty, r2_poverty = utils.predict_regression(emb, poverty)\n",
    "    \n",
    "    mae_value, rmse_value, r2_value = utils.predict_regression(emb, value)\n",
    "    mae_white, rmse_white, r2_white = utils.predict_regression(emb, white)\n",
    "    mae_black, rmse_black, r2_black = utils.predict_regression(emb, black)\n",
    "    mae_hispanic, rmse_hispanic, r2_hispanic = utils.predict_regression(emb, hispanic)\n",
    "        \n",
    "    print('----------------------------------------------------Results-----------------------------------------------------\\n')\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print('income prediction MAE:', mae_income.round(3))\n",
    "    print('income prediction RMSE:', rmse_income.round(3))\n",
    "    print('income prediction R2:', r2_income.round(3))\n",
    "    print('\\n')\n",
    "\n",
    "    print('unemployed prediction MAE:', mae_unemployed.round(3))\n",
    "    print('unemployed prediction RMSE:', rmse_unemployed.round(3))\n",
    "    print('unemployed prediction R2:', r2_unemployed.round(3))\n",
    "    print('\\n')\n",
    "\n",
    "    print('education prediction MAE:', mae_education.round(3))\n",
    "    print('education prediction RMSE:', rmse_education.round(3))\n",
    "    print('educationg prediction R2:', r2_education.round(3))\n",
    "    print('\\n')\n",
    "\n",
    "    print('poverty prediction MAE:', mae_poverty.round(3))\n",
    "    print('poverty prediction RMSE:', rmse_poverty.round(3))\n",
    "    print('poverty prediction R2:', r2_poverty.round(3))\n",
    "    print('\\n')\n",
    "\n",
    "    print('white prediction MAE:', mae_white.round(3))\n",
    "    print('white prediction RMSE:', rmse_white.round(3))\n",
    "    print('white prediction R2:', r2_white.round(3))\n",
    "    print('\\n')\n",
    "\n",
    "    print('black prediction MAE:', mae_black.round(3))\n",
    "    print('black prediction RMSE:', rmse_black.round(3))\n",
    "    print('black prediction R2:', r2_black.round(3))\n",
    "    print('\\n')\n",
    "\n",
    "    print('hispanic prediction MAE:', mae_hispanic.round(3))\n",
    "    print('hispanic prediction RMSE:', rmse_hispanic.round(3))\n",
    "    print('hispanic prediction R2:', r2_hispanic.round(3))\n",
    "    print('\\n')\n",
    "\n",
    "    print('value prediction MAE:', mae_value.round(3))\n",
    "    print('value prediction RMSE:', rmse_value.round(3))\n",
    "    print('value prediction R2:', r2_value.round(3))\n",
    "    print('\\n')\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8654a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce49add",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from model.ipynb\n",
      "total_params: 2816320\n",
      "Epoch 0100 | Loss = 501.5728\n",
      "Epoch 0200 | Loss = 498.8011\n",
      "Epoch 0300 | Loss = 498.2394\n",
      "Epoch 0400 | Loss = 497.6246\n",
      "Epoch 0500 | Loss = 497.0928\n",
      "Early stopping\n",
      "Epoch 0582 | Loss = 497.3006\n",
      "emb size: (308, 64)\n",
      "----------------------------------------------------Results-----------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "income prediction MAE: 9621.497\n",
      "income prediction RMSE: 12620.625\n",
      "income prediction R2: 0.57\n",
      "\n",
      "\n",
      "unemployed prediction MAE: 2.615\n",
      "unemployed prediction RMSE: 3.384\n",
      "unemployed prediction R2: 0.76\n",
      "\n",
      "\n",
      "education prediction MAE: 5.425\n",
      "education prediction RMSE: 7.17\n",
      "educationg prediction R2: 0.41\n",
      "\n",
      "\n",
      "poverty prediction MAE: 4.329\n",
      "poverty prediction RMSE: 5.923\n",
      "poverty prediction R2: 0.686\n",
      "\n",
      "\n",
      "white prediction MAE: 9.708\n",
      "white prediction RMSE: 13.083\n",
      "white prediction R2: 0.753\n",
      "\n",
      "\n",
      "black prediction MAE: 11.417\n",
      "black prediction RMSE: 15.951\n",
      "black prediction R2: 0.83\n",
      "\n",
      "\n",
      "hispanic prediction MAE: 12.456\n",
      "hispanic prediction RMSE: 17.102\n",
      "hispanic prediction R2: 0.611\n",
      "\n",
      "\n",
      "value prediction MAE: 65195.344\n",
      "value prediction RMSE: 88778.757\n",
      "value prediction R2: 0.649\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {'device': 'cuda:0', 'lr':1e-3, 'weight_decay':0, 'hidden_size':64,\n",
    "        'num_epochs':8000, 'dropout':0, 'num_heads':[10], 'grad_norm':2,'out_size': 64}      \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "emb = main(100, k=75, patience=100, args=args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
