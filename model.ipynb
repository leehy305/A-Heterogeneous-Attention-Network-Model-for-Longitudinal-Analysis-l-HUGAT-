{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1162624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import GATConv\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3807a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HUGAT_J(nn.Module):\n",
    "    def __init__(self, data, meta_paths, in_size, hidden_size, out_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.hugat1 = HUGAT(data['heterograph_unified{}'.format(2018)], meta_paths, in_size, hidden_size, out_size, num_heads, dropout)\n",
    "        self.hugat2 = HUGAT(data['heterograph_unified{}'.format(2019)], meta_paths, in_size, hidden_size, out_size, num_heads, dropout)\n",
    "        self.hugat3 = HUGAT(data['heterograph_unified{}'.format(2020)], meta_paths, in_size, hidden_size, out_size, num_heads, dropout)\n",
    "        self.hugat4 = HUGAT(data['heterograph_unified{}'.format(2021)], meta_paths, in_size, hidden_size, out_size, num_heads, dropout)\n",
    "        \n",
    "        '''meta-path for l-urban-HIN'''\n",
    "        self.tot_meta = [['zone-zone'],['src-time','time-src'],['dst-time','time-dst'],\n",
    "                         ['years-after'],['years-before'],['zone-year','year-zone']]\n",
    "\n",
    "        self.hugat_total = HUGAT(data['heterograph_unified_3'], self.tot_meta, out_size, hidden_size, out_size, num_heads, dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, data, h):\n",
    "        num_nodes = 77\n",
    "        emb2018, beta2018 = self.hugat1.forward(data['heterograph_unified{}'.format(2018)], h[num_nodes*0:num_nodes*1,:])\n",
    "        emb2019, beta2019 = self.hugat2.forward(data['heterograph_unified{}'.format(2019)], h[num_nodes*1:num_nodes*2,:])\n",
    "        emb2020, beta2020 = self.hugat3.forward(data['heterograph_unified{}'.format(2020)], h[num_nodes*2:num_nodes*3,:])\n",
    "        emb2021, beta2021 = self.hugat4.forward(data['heterograph_unified{}'.format(2021)], h[num_nodes*3:num_nodes*4,:])\n",
    "        \n",
    "        embs = torch.stack([emb2018, emb2019, emb2020, emb2021])\n",
    "        embs = torch.reshape(embs, (num_nodes*4,-1))\n",
    "        \n",
    "        emb_total, beta = self.hugat_total.forward(data['heterograph_unified_3'], embs)\n",
    "        \n",
    "        beta_whole = [beta2018, beta2019, beta2020, beta2021, beta]\n",
    "        \n",
    "        return emb_total, beta_whole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4e4ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "\n",
    "class HUGAT(nn.Module):\n",
    "    def __init__(self, g, meta_paths, in_size, hidden_size, out_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.han = HAN(meta_paths, in_size, hidden_size, out_size, num_heads, dropout)\n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        self.emb, beta = self.han.forward(g, h)        \n",
    "        return self.emb, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ede96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hellinger_pairwise(a, b):\n",
    "    hellinger_distance = ((1/2)**(1/2)) * torch.cdist(torch.sqrt(a), torch.sqrt(b), p=2)\n",
    "    return hellinger_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5eace56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "    def __init__(self, meta_paths, in_size, hidden_size, out_size, num_heads, dropout):\n",
    "        super(HAN, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(HANLayer(meta_paths, in_size, hidden_size, num_heads[0], dropout))\n",
    "        for l in range(1, len(num_heads)):\n",
    "            self.layers.append(HANLayer(meta_paths, hidden_size * num_heads[l-1],\n",
    "                                        hidden_size, num_heads[l], dropout))\n",
    "        self.predict = nn.Linear(hidden_size * num_heads[-1], out_size)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        i = 0\n",
    "        for gnn in self.layers:\n",
    "            if i < len(self.num_heads):\n",
    "                h, beta = gnn(g, h)\n",
    "            else:\n",
    "                h = gnn(g, h)\n",
    "            i += 1\n",
    "        return self.predict(h), beta   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31938e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_inner_product(a, b):\n",
    "    n, _ = list(a.size())\n",
    "    b_ = torch.unsqueeze(b, 0)\n",
    "    b_ = torch.tile(b_, [n, 1, 1])\n",
    "    b_ = b_.permute(1, 0, 2)\n",
    "    inner_product = torch.multiply(b_, a)\n",
    "    inner_product = torch.sum(inner_product, axis=-1)\n",
    "    return inner_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c672f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    HAN layer.\n",
    "    \n",
    "    Dimensions\n",
    "    ---------\n",
    "    N : number of nodes\n",
    "    D : dimension of output feature\n",
    "    M : cardinality of meta-pahts\n",
    "    K : number of Multi-heads\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    meta_paths : list of metapaths, each as a list of edge types\n",
    "    in_size : input feature dimension\n",
    "    out_size : output feature dimension\n",
    "    layer_num_heads : number of attention heads\n",
    "    dropout : Dropout probability\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    g : DGLHeteroGraph\n",
    "        The heterogeneous graph\n",
    "    h : tensor\n",
    "        Input features\n",
    "    Outputs\n",
    "    -------\n",
    "    tensor\n",
    "        The output feature\n",
    "    \"\"\"\n",
    "    def __init__(self, meta_paths, in_size, out_size, layer_num_heads, dropout):\n",
    "        super(HANLayer, self).__init__()\n",
    "\n",
    "        # One GAT layer for each meta path based adjacency matrix\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for i in range(len(meta_paths)):\n",
    "            self.gat_layers.append(GATConv(in_size, out_size, layer_num_heads,\n",
    "                                           dropout, dropout, activation=F.elu,\n",
    "                                           allow_zero_in_degree=True))\n",
    "        self.semantic_attention = SemanticAttention(in_size=out_size * layer_num_heads)\n",
    "        self.meta_paths = list(tuple(meta_path) for meta_path in meta_paths)\n",
    "\n",
    "        self._cached_graph = None\n",
    "        self._cached_coalesced_graph = {}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        semantic_embeddings = []\n",
    "\n",
    "        if self._cached_graph is None or self._cached_graph is not g:\n",
    "            self._cached_graph = g\n",
    "            self._cached_coalesced_graph.clear()\n",
    "            for meta_path in self.meta_paths:\n",
    "                self._cached_coalesced_graph[meta_path] = dgl.metapath_reachable_graph(\n",
    "                        g, meta_path)\n",
    "\n",
    "        for i, meta_path in enumerate(self.meta_paths):\n",
    "            new_g = self._cached_coalesced_graph[meta_path]\n",
    "            # concatenate\n",
    "            semantic_embeddings.append(self.gat_layers[i](new_g, h).flatten(1))\n",
    "        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)                  \n",
    "\n",
    "        return self.semantic_attention(semantic_embeddings)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871f5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticAttention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=128):\n",
    "        super(SemanticAttention, self).__init__()\n",
    "        '''\n",
    "        q.T x tanh(Wz+b)\n",
    "        '''\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)     \n",
    "        )                                              \n",
    "\n",
    "    def forward(self, z):                              \n",
    "        w = self.project(z).mean(0)                  \n",
    "        beta = torch.softmax(w, dim=0)                 \n",
    "        beta = beta.expand((z.shape[0],) + beta.shape) \n",
    "\n",
    "        return (beta * z).sum(1), beta                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
